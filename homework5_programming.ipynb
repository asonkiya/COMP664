{"cells":[{"cell_type":"markdown","metadata":{"id":"w96U8AVPPVjW"},"source":["# Homework 5 - Representation Learning and Word Embeddings\n","\n","In this assignment, you will explore word embeddings using Word2Vec and other techniques. You will also visualize the learned embeddings and perform various tasks to understand the properties and applications of word embeddings.\n","\n","You will:\n","1. Load pre-trained word embeddings using gensim.\n","2. Compute cosine similarity between word vectors.\n","3. Solve word analogy problems using word embeddings.\n","4. Visualize word embeddings using PCA.\n","5. Analyze the effect of singular/plural forms and comparative/superlative forms of words in the embedding space.\n","\n","\n","## Task 1: Load Pre-trained Word Embeddings (1 pt)\n","\n","You will load pre-trained word embeddings using the gensim library. You will also get the embedding for a specific word and check its dimensionality."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BwrQJH7PVjY"},"outputs":[],"source":["!pip install --upgrade gensim\n","import gensim.downloader as api\n","\n","# Load pre-trained word vectors\n","word_vectors = #TODO: Load pretrained embeddings from gensim\n","embedding_computer = #TODO: Get the embedding for the word 'computer'\n","\n","print(f\"Embedding for 'computer': {embedding_computer}\")\n","print(f\"Dimension of the embedding: {embedding_computer.shape[0]}\")"]},{"cell_type":"markdown","metadata":{"id":"MBwhQf71PVja"},"source":["## Task 2: Compute Cosine Similarity (1 pt)\n","\n","In this task, you will implement a function to compute the cosine similarity between two vectors from scratch (i.e., without using built-in functions except `math.sqrt()`). You will then use this function to compute the similarity between several pairs of words. Understanding cosine similarity is crucial for working with word embeddings and other vector representations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2SFpRQlPVja"},"outputs":[],"source":["import numpy as np\n","import math\n","\n","def cosine_similarity(vec1, vec2):\n","    # TODO: Implement function to compute cosine similarity between two embeddings vec1 and vec2\n","\n","# Compute similarity between pairs of words\n","words = [('computer', 'science'), ('boy', 'girl'), ('king', 'queen'), ('man', 'woman'), ('apple', 'orange'), ('cat', 'dog')]\n","for word1, word2 in words:\n","    vec1 = word_vectors[word1]\n","    vec2 = word_vectors[word2]\n","    similarity = cosine_similarity(vec1, vec2)\n","    print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity}\")"]},{"cell_type":"markdown","metadata":{"id":"6QjDRV_TPVja"},"source":["## Task 3: Solve Word Analogies (1 pt)\n","\n","In this task, you will use the word embeddings to solve word analogy problems. For example, you can solve the analogy `queen: woman :: ?:man` by finding the word that is most similar to the result of the operation `queen - woman + man`.\n","\n","Use gensim's built-in function `most_similar` to find the word most similar to the result of vector arithmetic operations\n","\n","This task illustrated the power of word embeddings in capturing semantic relationships."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4S2_z8GPVja"},"outputs":[],"source":["# Solve word analogies\n","analogies = [\n","    ('queen', 'woman', 'man'),\n","    ('dad', 'man', 'woman'),\n","    ('paris', 'france', 'italy'),\n","    ('sun', 'day', 'night'),\n","    ('tree', 'forest', 'river'),\n","    ('happy', 'joy', 'sad')\n","]\n","for word1, word2, word3 in analogies:\n","    #TODO: Solve the analogies above by finding the word most similar (your_answer) to the result of vector arithmetic operations described.\n","    print(f\"{word1} - {word2} + {word3} = {your_answer} \")"]},{"cell_type":"markdown","metadata":{"id":"y2qauB4uPVja"},"source":["## Task 4: Visualize Word Embeddings\n","\n","Next, we will visualize word embeddings using PCA (Principal Component Analysis). We will select a set of words, get their embeddings, and plot them in a 2D space. This visualization can help us in understanding the structure and relationships in the embedding space."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"onH3rQKSPVjb"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# List of words to visualize\n","words_to_visualize = ['king', 'queen', 'man', 'woman', 'boy', 'girl', 'computer', 'science', 'math', 'art', 'apple', 'orange', 'cat', 'dog', 'car', 'bicycle']\n","embeddings = [word_vectors[word] for word in words_to_visualize]\n","\n","# Reduce dimensionality\n","pca = PCA(n_components=2)\n","embeddings_2d = pca.fit_transform(embeddings)\n","\n","# Plot embeddings\n","plt.figure(figsize=(12, 8))\n","for i, word in enumerate(words_to_visualize):\n","    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1])\n","    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n","plt.xlabel('PCA Component 1')\n","plt.ylabel('PCA Component 2')\n","plt.title('2D Visualization of Word Embeddings')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"3qMaX6XFPVjb"},"source":["## Task 5: Analyze Singular and Plural Forms\n","\n","Next, we will analyze the singular and plural forms of several nouns and their relations in the embedding space. We will select a set of words, get their embeddings, and visualize them. This can help understand how embeddings represent morphological variations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gtPC0CXFPVjb"},"outputs":[],"source":["# List of singular and plural words\n","singular_plural_words = ['king', 'kings', 'queen', 'queens', 'man', 'men', 'woman', 'women', 'boy', 'boys', 'girl', 'girls', 'cat', 'cats', 'dog', 'dogs']\n","embeddings_singular_plural = [word_vectors[word] for word in singular_plural_words]\n","\n","# Reduce dimensionality\n","embeddings_singular_plural_2d = pca.fit_transform(embeddings_singular_plural)\n","\n","# Plot embeddings\n","plt.figure(figsize=(12, 8))\n","for i, word in enumerate(singular_plural_words):\n","    plt.scatter(embeddings_singular_plural_2d[i, 0], embeddings_singular_plural_2d[i, 1])\n","    plt.annotate(word, (embeddings_singular_plural_2d[i, 0], embeddings_singular_plural_2d[i, 1]))\n","plt.xlabel('PCA Component 1')\n","plt.ylabel('PCA Component 2')\n","plt.title('2D Visualization of Singular and Plural Word Embeddings')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ZTiXOy0EPVjb"},"source":["## Task 6: Analyze Adjectives and Their Forms\n","\n","Next, we will analyze the comparative and superlative forms of several adjectives and their relations in the embedding space. We will select a set of words, get their embeddings, and visualize them.\n","\n","This can help us in understanding how word embeddings capture comparative and superlative relations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAdMO8hqPVjb"},"outputs":[],"source":["# List of adjectives and their comparative and superlative forms\n","adjectives = ['good', 'better', 'best', 'bad', 'worse', 'worst', 'slow', 'slower', 'slowest', 'fast', 'faster', 'fastest', 'happy', 'happier', 'happiest', 'sad', 'sadder', 'saddest']\n","embeddings_adjectives = [word_vectors[word] for word in adjectives]\n","\n","# Reduce dimensionality\n","embeddings_adjectives_2d = pca.fit_transform(embeddings_adjectives)\n","\n","# Plot embeddings\n","plt.figure(figsize=(12, 8))\n","for i, word in enumerate(adjectives):\n","    plt.scatter(embeddings_adjectives_2d[i, 0], embeddings_adjectives_2d[i, 1])\n","    plt.annotate(word, (embeddings_adjectives_2d[i, 0], embeddings_adjectives_2d[i, 1]))\n","plt.xlabel('PCA Component 1')\n","plt.ylabel('PCA Component 2')\n","plt.title('2D Visualization of Adjective Word Embeddings')\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","source":["## Summary (1 pt)\n","\n","Briefly summarize your qualitative findings from Tasks 4, 5 and 6."],"metadata":{"id":"Bj5G10ZUm7fq"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}